import os
from typing import Dict, List, Tuple, Optional

import numpy as np
import skbio
import torch
from enum import Enum
from ete3 import Tree
from Bio import SeqIO
from torch.utils.data import Dataset

# Define the alphabet for nucleotides and amino acids
NUCLEOTIDES_LIST = np.array(list("ATGC")) # used for our alignment example
AMINO_ACIDS_LIST = np.array(list("ARNDCQEGHILKMFPSTWYVX-"))

# Enum class to define the data type
class DataType(Enum):
    NUCLEOTIDES = len(NUCLEOTIDES_LIST)
    AMINO_ACIDS = len(AMINO_ACIDS_LIST)
    TYPING = 32

class TensorDataset(Dataset):
    """A Dataset class to train ML4Phylo networks"""

    def __init__(self, directory: str, filter: Optional[List[str]] = None):
        """Instanciates a TensorDataset

        Parameters
        ----------
        directory : str
            Path to the directory containing .tensor_pair files generated by the
            `make_tensors` script.
        filter: List[str], optional
            List of tensor pair names to keep (useful if you keep training and
            validation tensors in the same directory), default is None

        Returns
        -------
        TensorDataset
            A instance of TensorDataset for training ML4Phylo
        """
        super(TensorDataset, self).__init__()
        self.directory = directory
        self.pairs = [
            filepath
            for filepath in os.listdir(self.directory)
            if filepath.endswith(".tensor_pair")
        ]
        if filter is not None:
            self.pairs = [id for id in self.pairs if id in filter]

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, index: int):
        pair = torch.load(os.path.join(self.directory, (self.pairs[index])))
        return pair["X"], pair["y"]


def load_data(path: str, data_type = DataType.AMINO_ACIDS) -> Tuple[torch.Tensor, List[str]]:
    """Loads an alignment into a tensor digestible by the Ml4Phylo network

    Parameters
    ----------
    path : str
        Path to a fasta file containing the alignment

    Returns
    -------
    Tuple[torch.Tensor, List[str]]
        a tuple containing:
         - a tensor representing the alignment (shape encoding_size * data_len * n_data)
         - a list of ids of the sequences in the alignment
    """
    # Check the size of the encoding for the corresponding data type
    tensor_list = []
    parsed = _parse_typing(path) if data_type == DataType.TYPING else _parse_alignment(path)

    # Iterate over all the data present in the dictionary
    for sequence in parsed.values():
        """
            Encodes every sequence obtaining a matrix with 2 dimensions (data_length, encoding_size)
            This matrix stores binary values that represent for each char of the sequence its corresponding
            amino acid or nucleotide and for the typing data the binary encoding of the genome identifier.
        """
        encoded_sequence = _binary_encoding(sequence) if data_type == DataType.TYPING else _sequence_to_one_hot(sequence, data_type)

        # Creates a tensor from the encoded sequence inverting his dimension to (encoding_size, data_length)
        tensor = torch.from_numpy(encoded_sequence).t()

        # Reshapes the tensor to a 3-dimensional one
        reshaped_tensor = tensor.view(data_type.value, 1, -1)

        tensor_list.append(                                              
            reshaped_tensor
        )

    """
        Concats all the tensors present in the list.
        As tensors are made up of 3 dimensions (encoding_size, 1, data_length), it presents (encoding_size) matrixes.
        After the concatenation the obtained tensor has matrixes with dimension (n_data, data_length), leading to
        a tensor of dimensions (encoding_size, n_data, data_length).
    """
    concated_tensors = torch.cat(tensor_list, dim=1)

    """
        Finally, the transpose of the last two dimensions is performed,
        resulting in a tensor of dimensions (encoding_size, data_length, n_data).
    """
    final_tensor = concated_tensors.transpose(-1, -2)

    return final_tensor, list(parsed.keys())


def _parse_alignment(path: str) -> Dict[str, str]:
    """Parses one fasta alignment

    Parameters
    ----------
    path : str
        Path to .fasta alignment file

    Returns
    -------
    Dict[str,str]
        A dictionnary with ids as keys and sequence as values
    """
    return {record.id: str(record.seq) for record in SeqIO.parse(path, format="fasta")}


def _sequence_to_one_hot(seq: str, data_type: DataType) -> np.ndarray:
    """Encode a sequence with one-hot encoding

    Parameters
    ----------
    seq : str
        Sequence to encode

    Returns
    -------
    np.ndarray
        Encoded sequence (shape encoding_size * data_len)
    """
    alphabet = NUCLEOTIDES_LIST if data_type == DataType.NUCLEOTIDES else AMINO_ACIDS_LIST
    return np.array([(alphabet == char).astype(int) for char in seq])


def _parse_typing(path: str) -> Dict[str, list]:
    """Parses one txt typing data file and calculates the encoding size

    Parameters
    ----------
    path : str
        Path to .txt typing data file

    Returns
    -------
    Dict[str,str]
        A dictionary with ids as keys and genome identifiers as values
    """
    with open(path, 'r') as f:
        records = f.readlines()[1:]

    rec_dict = {}

    for rec in (record.split() for record in records):
        int_geneId_list = [int(i) for i in rec[1:]]
                
        rec_dict[rec[0]] = int_geneId_list
                    
    return rec_dict


def _binary_encoding(seq: list) -> np.ndarray:
    """Encode a sequence with binary encoding

    Parameters
    ----------
    seq : str
        Sequence to encode
        
    encoding_size : int
        Encoded value size

    Returns
    -------
    np.ndarray
        Encoded sequence (shape encoding_size * data_len)
    """
    return np.array([list('{0:0{1}b}'.format(n, DataType.TYPING.value)) for n in seq]).astype(int)


def load_tree(path: str) -> Tuple[torch.Tensor, List[Tuple[str, str]]]:
    """Loads a tree as a tensor of pairwise distances, digestible by the ML4Phylo
    network

    Parameters
    ----------
    path : str
        Path to the newick file containing the tree

    Returns
    -------
    Tuple[torch.Tensor, List[Tuple[str, str]]]
        a tuple containing:
         - a tensor representing the distance matrix of the tree (shape 1\*n_pairs)
         - a list of tuples of ids indicating between which leafs the distance was
           computed

    """
    distances = _read_distances_from_tree(path)

    tensor, ids = [], []
    for pair, distance in distances.items():
        tensor.append(distance)
        ids.append(pair)

    return (torch.tensor(tensor), ids)


def _read_distances_from_tree(
    path: str, normalize: bool = False
) -> Dict[Tuple[str, str], float]:
    """Reads a phylogenetic tree and returns the corresponding distance matrix

    Parameters
    ----------
    path : str
        Path to the newick file containing the tree
    normalize : bool, optional
        Wether to normalize distances or not, by default False

    Returns
    -------
    Dict[Tuple[str, str], float]
        A dictionary representing the triangular distance matrix with:
         - as keys: a tuple of the leaf ids between which the distance is computed
         - as values: the distances

    """

    """
        The tree is iterated with the objective to calculate all the distances in the tree and register them in a dictionary.
        The ete3 function [leaf1.get_distance(leaf2)] permits the calculation of the distance between 2 nodes/leafs.
    """
    tree = Tree(path)
    distances = dict()
    for i, leaf1 in enumerate(tree):
        for j, leaf2 in enumerate(tree):
            if i < j:
                distances[(leaf1.name, leaf2.name)] = leaf1.get_distance(leaf2)

    """
        Finally, all the distances of the tree are normalized if necessary.
        This normalization process, finds the maximum distance value in the dict and divides every other
        distance by this maximum value.
        This is done to scale all the distances so that they fall between 0 and 1.
        Its a useful method for comparing different trees, as it puts the distances on a consistent scale.
    """
    if normalize:
        diameter = max(distances.values())
        for key in distances:
            distances[key] /= diameter

    return distances


def write_dm(dm: skbio.DistanceMatrix, path: str):
        """Write a distance matrix to disk in the square Phylip matrix format

        Parameters
        ----------
        dm : skbio.DistanceMatrix
            Distance matrix to save
        path : str
            Path where to save the matrix
        """

        with open(path, "w+") as file:
            file.write(f"{len(dm.ids)}\n")
            for id, dists in zip(dm.ids, dm.data):
                line = " ".join(str(dist) for dist in dists)
                file.write(f"{id}     {line}\n")